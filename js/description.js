d3.select("#description")
    .append("div")
        .attr("id", "description-top-button-container")
        .on("click", function() {
            document.body.scrollTop = 0; // For Safari
            document.documentElement.scrollTop = 0; // For Chrome, Firefox, IE and Opera
        })
d3.select("#description-top-button-container")
    .append("img")
        .attr("src", "icons/top.svg")
        .attr("id", "description-top-button-img")
d3.select("#description-top-button-container")
    .append("div")
        .attr("id", "description-top-button-text")
        .text("top")
d3.select("#description")
    .append("div")
        .attr("id", "description-section-what")
        .attr("class", "description-sec")
        .append("h1")
            .text("Stable Diffusion是什么?")
d3.select("#description-section-what")
    .append("p")
        .html(`
        Stable Diffusion is a text-to-image model that transforms a text prompt into a high-resolution image. 
        For example, if you type in 
        <span style="color: var(--text3);">a cute and adorable bunny</span>, 
        Stable Diffusion generates high-resolution images depicting that 
        &mdash; <span style="color: var(--text3);">a cute and adorable bunny</span> &mdash; 
        in a few seconds. 
        Click “Select another prompt” in Diffusion Explainer to change prompts and check the fascinating images generated from each prompt!`)

// How does Stable Diffusion work?
d3.select("#description")
    .append("div")
        .attr("id", "description-section-how-work")
        .attr("class", "description-sec")
        .append("h1")
            .text("Stable Diffusion如何工作?")
d3.select("#description-section-how-work")
    .append("p")
        // .text('Stable Diffusion first generates a vector representation of an image depicted in the text prompt. This image representation is then upscaled into a high-resolution image.')
        .html(`
        Stable Diffusion first changes the text prompt into a <span style="font-style: italic">text representation</span>, 
        numerical values that summarize the prompt. 
        The text representation is used to generate an <span style="font-style: italic">image representation</span>, 
        which summarizes an image depicted in the text prompt. 
        This image representation is then upscaled into a high-resolution image.
        `)
d3.select("#description-section-how-work")
    .append("p")
        // .html('You may wonder why Stable Diffusion introduces image representation instead of directly generating high-resolution images. The reason is <span style="font-style: italic;">computational cost efficiency</span>. Doing most computations on representation, which summarizes an image in a compact form, significantly reduces the computational cost while maintaining high image quality.')
        .html(`
        You may wonder why Stable Diffusion introduces image representation instead of directly generating high-resolution images. 
        The reason is <span style="font-style: italic">computational efficiency</span>. 
        Doing most computations on compact image representation instead of a high-resolution image significantly reduces the time and cost for the computations while maintaining high image quality.
        `)
d3.select("#description-section-how-work")
    .append("p")
        // .html('The image representation starts as a random noise and is refined over multiple timesteps to reach the image representation for your text prompt. The number of timesteps is a hyperparameter determined before refining and typically set to 50.')
        .html(`
        The image representation, which starts as a random noise, 
        is refined over multiple timesteps to reach the image representation for a high-quality image with strong adherence to the text prompt. 
        The number of refining timesteps is typically set as 50 or 100; we fix it to 50 in Diffusion Explainer.
        `)
d3.select("#description-section-how-work")
    .append("p")
        .html('We break down the image generation process of Stable Diffusion into three main steps:')
        .append("ol")
        .attr("id", "description-generation-main-steps-ol")
d3.select("#description-generation-main-steps-ol")
    .append("li")
        .html('<a style="font-weight: 500" href="#description-subsec-text-representation-generation">Text Representation Generation</a>: Stable Diffusion converts a text prompt into a text vector representation.')
d3.select("#description-generation-main-steps-ol")
    .append("li")
        .html('<a style="font-weight: 500" href="#description-subsec-image-representation-refining">Image Representation Refining</a>: Starting with random noise, Stable Diffusion refines the image representation little by little, with the guidance of the text representation. Stable Diffusion repeats the refining over multiple timesteps (50 in our Diffusion Explainer).')
d3.select("#description-generation-main-steps-ol")
    .append("li")
        .html('<a style="font-weight: 500" href="#description-subsec-image-upscaling">Image Upscaling</a>: Stable Diffusion upscales the image representation into a high-resolution image.')
d3.select("#description-section-how-work")
    .append("p")
        .text("Now, let's look closer into each process.")

// Text Representation Generation
d3.select("#description-section-how-work")
    .append("div")
        .attr("id", "description-subsec-text-representation-generation")
        .attr("class", "description-subsec")
        .append("h2")
            .text("文本表示生成(Text Representation Generation)")
d3.select("#description-subsec-text-representation-generation")
    .append("img")
        .attr("class", "description-gif")
        .attr("id", "text-representation-generation-expansion-gif")
        .attr("src", "assets/gif/trg.gif")
d3.select("#description-subsec-text-representation-generation")
    .append("p")
        // .text("Text representation generation consists of tokenizing and text encoding.")
        .html(`
        Clicking Text Representation Generation shows how a text prompt is converted into a text representation, 
        a vector that summarizes the prompt. 
        It consists of two steps: 
        <span style="font-style: italic">tokenizing</span> 
        and 
        <span style="font-style: italic">text encoding</span>.`)
// Tokenizing
d3.select("#description-subsec-text-representation-generation")
    .append("div")
        .attr("id", "description-subsubsec-tokenizing")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-tokenizing")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('1. Tokenizing')
d3.select("#description-subsubsec-tokenizing")
    .append("p")
        // .html("Tokenizing is a common way to handle text input to standardize the format of the input and enable the text input to be processed by neural networks.")
        .html(`Tokenizing is a common way to handle text data to change the text into numbers and process them with neural networks.`)
d3.select("#description-subsubsec-tokenizing")
    .append("div")
        .attr("class", "description-paragraph")
        .attr("id", "description-subsubsec-tokenizing-token-example-paragraph")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
    .html(`Stable Diffusion tokenizes a text prompt into a sequence of tokens. 
    For example, it splits the text prompt `)
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .style("color", "var(--text3)")
        .text("a cute and adorable bunny ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text("into the tokens ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("a")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("cute")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("and")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("adorable")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", and ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("bunny")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(". Also, to mark the beginning and end of the prompt, Stable Diffusion adds ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<start>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(" and ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<end>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(" tokens at the beginning and the end of the tokens. The resulting token sequence for the above example would be ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<start>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("a")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("cute")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("and")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("adorable")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("bunny")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(", and ")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .attr("class", "text-vector-generator-token description-token")
        .text("<end>")
d3.select("#description-subsubsec-tokenizing-token-example-paragraph")
    .append("span")
        .text(".")

d3.select("#description-subsubsec-tokenizing")
    .append("p")
        // .html('To ensure that all token sequences have the same length for easier computation, Stable Diffusion pads or truncates the token sequences to exactly 77 tokens. If the input prompt has fewer than 77 tokens, <span class="text-vector-generator-token description-token" id="description-token-end"></span> tokens are added to the end of the sequence until it reaches 77 tokens. If the input prompt has more than 77 tokens, the last 77 tokens are retained and the rest are truncated. The number of tokens was set to balance performance and computational efficiency.')
        .html('For easier computation, Stable Diffusion keeps the token sequences of any text prompts to have the same length of 77 by padding or truncating. If the input prompt has fewer than 77 tokens, <span class="text-vector-generator-token description-token" id="description-token-end"></span> tokens are added to the end of the sequence until it reaches 77 tokens. If the input prompt has more than 77 tokens, the first 77 tokens are retained and the rest are truncated. The length of 77 was set to balance performance and computational efficiency.')
d3.select("#description-token-end").text("<end>")
// Text encoding
d3.select("#description-subsec-text-representation-generation")
    .append("div")
        .attr("id", "description-subsubsec-text-encoding")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-text-encoding")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('2. Text encoding')
d3.select("#description-subsubsec-text-encoding")
    .append("p")
        .html(`Stable Diffusion converts the token sequence into a text representation. To use the text representation for guiding image generation, Stable Diffusion ensures that the text representation contains the information related to the image depicted in the prompt. This is done by using a special neural network called <a href="https://openai.com/research/clip">CLIP</a>.`)
d3.select("#description-subsubsec-text-encoding")
    .append("p")
        .html("CLIP, which consists of an image encoder and a text encoder, is trained to encode an image and its text description into vectors that are similar to each other. Therefore, the text representation for a prompt computed by CLIP’s text encoder is likely to contain information about the images described in the prompt. You can display the visual explanations by clicking the Text Encoder above.")

// Image Representation Refining
d3.select("#description-section-how-work")
    .append("div")
        .attr("id", "description-subsec-image-representation-refining")
        .attr("class", "description-subsec")
        .append("h2")
            .text("图像表示优化(Image Representation Refining)")
d3.select("#description-subsec-image-representation-refining")
    .append("img")
        .attr("class", "description-gif")
        .attr("id", "image-refining-description-gif")
        .attr("src", "assets/gif/irr.gif")
d3.select("#description-subsec-image-representation-refining")
    .append("p")
        .html("Stable Diffusion generates image representation, a vector that numerically summarizes a high-resolution image depicted in the text prompt. This is done by refining a randomly initialized noise over multiple timesteps to gradually improve the image quality and adherence to the prompt. You can change the initial random noise by adjusting the <span style='font-style: italic;'>seed </span> in Diffusion Explainer. Click Image Representation Refiner to visualize each refinement step, which involves noise prediction and removal.")
// Noise Prediction
d3.select("#description-subsec-image-representation-refining")
    .append("div")
        .attr("id", "description-subsubsec-noise-prediction")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-noise-prediction")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('1. Noise Prediction')
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .html("At each timestep, a neural network called UNet predicts noise in the image representation of the current timestep. UNet takes three inputs:")
d3.select("#description-subsubsec-noise-prediction")
    .append("ol")
        .attr("id", "description-unet-input-ol")
d3.select("#description-unet-input-ol")
    .append("li")
    .html(`<span style="font-weight: 500;">Image representation</span> of the current timestep`)
d3.select("#description-unet-input-ol")
    .append("li")
    .html(`<span style="font-weight: 500; color: var(--text3);">Text representation</span> of the prompt to guide what noise should be removed from the current image representation to generate an image adhering to the text prompt`)
d3.select("#description-unet-input-ol")
    .append("li")
    .html(`<span style="font-weight: 500;">Timestep</span> to indicate the amount of noise remaining in the current image representation`)

d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .html(`In other words, 
        UNet predicts a <span style="color: var(--text3);">prompt-conditioned noise</span> in the current image representation 
        under the guidance of the text prompt's representation and timestep.`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .html(`However, even though we condition the noise prediction with the text prompt, 
        the generated image representation usually does not adhere strongly enough to the text prompt. 
        To improve the adherence, 
        Stable Diffusion measures the impact of the prompt by additionally predicting <span style="color: #909090;">generic noise conditioned on an empty prompt (" ")</span>
        and subtracting it from the prompt-conditioned noise:`) 
        // The final noise prediction is a weighted sum of the predicted 
        // <span style="color: #a0a0a0;">generic noise</span> and the
        // <span style="color: var(--text3);">prompt-conditioned noise</span>
        // with the weights controlled by the hyperparameter <span style="font-weight: 500;">guidance scale</span>:`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .attr("class", "description-equation")
        .html(`
        <span class="description-equation-term" style="color: var(--text3); background-color: #4d922110;">impact of prompt</span> 
        <span class="description-equation-op">=</span> 
        <span class="description-equation-term" style="color: var(--text3); background-color: #4d922110;">prompt-conditioned noise</span> 
        <span class="description-equation-op">-</span> 
        <span class="description-equation-term" style="color: #909090; background-color: #a0a0a020;">generic noise</span>`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
    .html(`
    In other words, the generic noise contributes to better image quality, 
    while the impact of the prompt contributes to the adherence to the prompt. 
    The final noise is a weighted sum of them controlled by a value called <span style="color: var(--text3);">guidance scale</span>:`)
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
        .attr("class", "description-equation")
        .attr("id", "description-equation-gs")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .style("color", "#909090")
        .text("generic noise")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-op")
        .text(" + ")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#27641910")
        .style("color", "var(--text3)")
        .text("guidance scale")
d3.select("#description-equation-gs")
    .append("span")
        .attr("class", "description-equation-op")
        .text(" x ")
d3.select("#description-equation-gs")
    .append("span")
        .style("background-color", "#27641910")
        .style("color", "var(--text3)")
        .attr("class", "description-equation-term")
        .text("impact of prompt")
d3.select("#description-subsubsec-noise-prediction")
    .append("p")
    .html(`A guidance scale of 0 means no adherence to the text prompt, 
    while a guidance scale of 1 means using the original prompt-conditioned noise. 
    Larger guidance scales result in stronger adherence to the text prompt, 
    while too large values can lower the image quality. 
    Change the guidance scale value in Diffusion Explainer and see how it changes the generated images.`)

// Noise Removal
d3.select("#description-subsec-image-representation-refining")
    .append("div")
        .attr("id", "description-subsubsec-noise-removal")
        .attr("class", "description-subsubsec")
d3.select("#description-subsubsec-noise-removal")
    .append("div")
        .attr("class", "description-subsubsec-title")
        .html('2. Noise Removal')
d3.select("#description-subsubsec-noise-removal")
    .append("p")
        .html("Stable Diffusion then decides how much of the predicted noise to actually remove from the image, as determined by an algorithm called scheduler. Removing small amounts of noise helps refine the image gradually and produce sharper images.")
d3.select("#description-subsubsec-noise-removal")
    .append("p")
        .html("The scheduler makes this decision by accounting for the total number of timesteps. The downscaled noise is then subtracted from the image representation of the current timestep to obtain the refined representation, which becomes the image representation of the next timestep:")
d3.select("#description-subsubsec-noise-removal")
    .append("p")
        .attr("class", "description-equation")
        .attr("id", "description-equation-denoise")
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .html(`image representation of timestep <span style="font-style: italic;">t+1</span>`)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-op")
        .html(` = `)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .html(`image representation of timestep <span style="font-style: italic;">t</span>`)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-op")
        .html(` - `)
d3.select("#description-equation-denoise")
    .append("span")
        .attr("class", "description-equation-term")
        .style("background-color", "#a0a0a020")
        .html(`downscaled noise`)

// Image Upscaling
d3.select("#description-section-how-work")
    .append("div")
        .attr("id", "description-subsec-image-upscaling")
        .attr("class", "description-subsec")
            .append("h2")
                .text("图像上采样(Image Upscaling)")
d3.select("#description-subsec-image-upscaling")
    .append("img")
    .attr("class", "description-gif")
    .attr("src", "assets/gif/upscale.gif")
d3.select("#description-subsec-image-upscaling")
    .append("p")
        .text("After all denoising steps have been completed, Stable Diffusion uses a neural network called Decoder to upscale the image representation into a high-resolution image. The refined image representation fully denoised with the guidance of the text representations would result in a high-resolution image strongly adhering to the text prompt.")

// Comparison View
d3.select("#description")
    .append("div")
        .attr("id", "description-section-comparison")
        .attr("class", "description-sec")
        .append("h1")
            .text("How do prompt keywords affect image generation?")
            .style("margin-bottom", "0.5em")
d3.select("#description-section-comparison")
    .append("img")
        .attr("class", "description-gif")
        .attr("id", "rcv-expansion-gif")
        .attr("src", "assets/gif/rcv.gif")
d3.select("#description-section-comparison")
    .append("p")
        .html(`
        编写提示词是一个需要不断尝试和重复的过程。
        例如，从简单的提示词
        <span style="color: var(--text3); font-style: italic;">可爱的兔子(a cute bunny)</span>
        开始，你需要反复添加和删除关键词，比如
        <span style="color: var(--text3); font-style: italic;">皮克斯可爱角色风格(in the style of cute pixar character)</span>，
        直到得到你想要的图像。`)
d3.select("#description-section-comparison")
    .append("p")
        .text(`
        因此，了解提示词关键字如何影响图像生成对编写和优化提示词非常有帮助。
        点击提示词中高亮的关键字，对比包含和不包含这些关键字的两个提示词生成的图像.`)

// What can we change
d3.select("#description")
    .append("div")
        .attr("id", "description-section-change")
        .attr("class", "description-sec")
        .append("h1")
            .text("What can we change?")
d3.select("#description-section-change")
    .append("p")
        .text("You have control over text prompt and hyperparameters in our Diffusion Explainer to change the generated images:")
d3.select("#description-section-change")
    .append("ul")
        .attr("id", "description-hyperparameter-ol")
d3.select("#description-hyperparameter-ol")
    .append("li")
    .html(`Text prompt: Description of the image you want to generate. A more detailed text prompt generally leads to better quality images.`)
d3.select("#description-hyperparameter-ol")
    .append("li")
    .html(`Seed: Random seed for the initialization of the image representation at timestep 0. Changing the seed will result in different image representation at timestep 0 and therefore different images.`)
d3.select("#description-hyperparameter-ol")
    .append("li")
    .html(`Guidance scale: How closely the generated image adheres to the text prompt. Increasing the guidance scale leads to stronger adherence but may make the generated images overly exaggerated.`)
d3.select("#description-section-change")
    .append("p")
        .text("Additionally, there are other hyperparameters that are not included in the Diffusion Explainer, such as the total number of timesteps, image size, and the type of scheduler.")



// How implemented?
d3.select("#description")
    .append("div")
        .attr("id", "description-section-who")
        .attr("class", "description-sec")
        .append("h1")
            .text("谁开发了Diffusion Explainer?")
d3.select("#description-section-who")
    .append("p")
        .html(`这个演示来自Git上的开源项目Diffusion Explainer，它由 
        <a href="http://www.seongmin.xyz">Seongmin Lee</a>, 
        <a href="https://bhoov.com">Ben Hoover</a>, 
        <a href="http://hendrik.strobelt.com">Hendrik Strobelt</a>, 
        <a href="https://zijie.wang">Jay Wang</a>, 
        <a href="https://shengyun-peng.github.io">Anthony Peng</a>, 
        <a href="https://www.austinpwright.com">Austin Wright</a>, 
        <a href="https://www.linkedin.com/in/kevinyli/">Kevin Li</a>, 
        <a href="https://haekyu.com">Haekyu Park</a>, 
        <a href="https://alexanderyang.me">Alex Yang</a>, 
        和 
        <a href="https://poloclub.github.io/polochau/">Polo Chau</a>一起开发.`)
